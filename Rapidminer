## Motivation: why data mining?
## What is data mining?
## Data Mining: on what kind of data?
## Data functionality
## Classification of data mining systems
## Major issues in data mining
 
It has the most powerful and easiest tool for data evaluation and an intelligent graphic for designing analysis processes for users. In this software, you can run R, and PYTHON codes specifically for you. It also can integrate with many other data mining software, such as Excel, Excel, Oracle, SQL, and many other software.
• Software to prepare data and provide suitable data for modeling in the shortest time.
• Perform modeling using machine learning algorithms and make accurate and correct predictions.
• Validate and validate the modeling results and show in this section which model is the best model for data prediction.
• Not suitable for working with large amounts of data. Allows tapping processes for users on web pages.
The repository part is the storage place for data processes and final models. In the operator part, there are various operators to form our process and build the model. In the upper right section, parameters: we have several parameters for each operator. Until we specify the parameters, we cannot continue the process our modeling does not take place. And in the bottom part, there is the help of the software.
This software includes two views that we design processes in the design section. And in the result part, the results of our processes and modeling are placed in this part.

1) How to enter the data into RapidMiner and store them in the repository. and later bring it to the design environment and do the modeling on them. There are two ways to enter data. 1. Let's use the sample part and the data part, a series of databases are stored here for teaching and learning. 2. We have a database in our computer and we want to enter it into our Rapidminer environment. We have a series of external data and we want them. Enter the rapid-miner environment.
To have a database, we enter the Rapidminer website and from there, we prepare a data set called Titanic. We save the desired Excel file. Now I want to call this file to the Rapidminer environment. We go to the add data section and select the My computer option. Let's search the file name and its format. Usually, everyone chooses the first one as the header in the data file. In the next step, we can tick Rapidminer, it will delete the examples whose data is lost. Another way is to choose what to do with the lost data, which is more basic and better. So we don't tick. Here in the first column, we can make changes to the selected features. Polynominal, multi-quantity data, binominal, and two-quantity data, the next option, change the role, determine the role for the data, exclude the column, and remove the column from the data.
In Rapidminer, we can work with more numerical data. In the next part, we have to specify the name of our file and where it should be saved. Here, when we finish, the software enters the result menu by default. Now we go to the design environment
and we want to drag and drop the dataset saved in the local repository into our design environment. Each of these databases and operators has a port, and the data that enters the operators has an output port. By clicking on the output port, we connect the created line to the result part. And we climb up from the blue triangle. And we see the data in this section. There are several options in the filter section: one is to not show the ones with missing attributes - to show exactly the missing ones - to remove the missing labels - to show the missing labels. Here, since we didn't add a label, we chose the same part as all.
The next part in the left menu: is statistics: we can do a simple analysis and see our data in another way, charts can also be displayed here.
In the visualization section, we can draw all kinds of graphs. We select x: Age, y: Survived, and gender in the color column. Here we can have an analysis. Men survived longer. In the training part, we want to use the datasets and filter the women and finally see who paid the most for the boat rental. We enter the Titanic dataset from the local repository section. In Rapidminer, we know our lines as examples, and in data mining as tuples, or they are statistical samples. The data table is called a dataset. In this part, we want to completely separate the men and leave the women and see who paid, by sorting the passenger fare attribute, we can determine who paid the most for the ship fare, we return to the design environment, filter example operator We enter it into the environment and drag it. We will bring it to the design page. We connect our dataset to the input port of the filter example. until later when we want to define parameters for the filter example. Do not get confused. If we click once on each operator, its parameters will be displayed in the right part. To add a filter, we click on the add filter part. The first part is gender, the second part is equality, and the last part is Female with a capital letter. Uppercase and lowercase letters are sensitive and we do OK. If we want to add more filters, we can do so from the add-entry section.
The description of the output ports of the filter example: The first part shows our dataset after the operation. The second part, if necessary, displays the original dataset, and the third part shows all the data other than the ones that have been operated on - that is, all the data other than the ones that have been operated on - if we click the option here We see that in the sex section, all the women have been separated and brought to us, and our example has reached 466. In the search section, find an example sort drag it to the process environment, and connect the ports. By clicking once on the sort operator, we can adjust its parameters. Based on the passenger fare and burning, it should be from high to low.
After connecting the port, we run it to see the results. The highest passenger fare is 512. To get information about each operator, we right-click and select the show operator option. Another thing is to right-click and turn the operator from active to inactive.
2) Project 2: In this part, we talk about data merging and grouping. In the samples-data section, there is a product dataset that we drag and a transaction dataset.

Here, we bring these two products and transactions connect their ports, and check the results and data status of each, then disconnect the ports and drag the join from search operators and all three and the port We connect them. Solution We need to determine according to which attribute these two are connected, so we remove the use id attribute as the key option from the selection mode and edit the list. So we select the product ID on both sides of the joining point of both IDs. And we hit the app. In this part, we run once - now we see that we have a bigger table that includes all the components of the previous two tables. To know which of our products had the most sales, as we can see in the results section, the product ID has been repeated several times, so these must be grouped first, then their total sales will be summed up and displayed. So we return to the design environment and this time we search and drag the aggregate operator. Here we want to add another function to our aggregate. In the name of the product name, we edit the list and click on Add Entry, select the product name, and set the opposite option to mode. Because the format is a string, the mod will bring us all the names of the products, and we will apply them at the end. And finally, we connect from the aggregate port to the result port. And we run. Integration and mixing of data is done in the pre-processing stage, where the product name-product ID and the number of buyers of that product are determined.

3) Project 3 and 4: In this part, we are going to insert a new column instead of two columns and remove useless columns from our dataset. So far we have learned to work with several operators for data preprocessing. We want to change our dataset to have a more suitable set for forecasting and modeling. It is to reduce the volume of useless data and use more suitable data. Instead of separately having the price of the products and the number of products purchased, we will have the total income from the sale of both in one column. Here again, we bring two products and transactions and connect them through join and run. We bring the generateattributes operator select the first option total revenue in the edit list, and select the second option from the calculator section, where different functions are defined, from the multiplication operator between the amount and the price. We connect the port to it. By executing this section, we notice that a column has been added. Total revenue will be added, but the price and amount columns will be deleted. We return to the design section and select only the attributes we have. After we use the select attribute operator, which is a very widely used operator, here we want to select only the attributes we use. After the attribute filter type section, which has various choices, we select sunset, then we press select attribute and select the attributes customer ID-product name-total revenue and apply. With the implementation of this step, we will need three columns, the highest amount of our income will be determined by which product was related to it. Now we want to know which product has the most sales, so we return to the design environment and use the Aggregate operator. Because we want the income according to the type of product, we want it to be grouped in this section. In the edit list: total revenue section, we want it to sum up the name of the products for us. And finally, we ran out of our collection. Based on the name of the products, the amount of income earned from those products is determined for us.

4) Project 5: Here we are going to work on changing types and roles. It means to change the format of some of our data. and set labels for some attributes. Here we learn how to change data format using operators. We specify the role of that attribute. For machine learning, the role of each attribute must be distinct - those known as regular roles are used for learning algorithms. Attributes that play the role of ID are generally ignored in the modeling algorithm. The attributes that have the role of label are considered as the attributes that are supposed to be predicted. We enter the discretize by Binning operator into the environment and set our attribute to the age of people. Binning is the technique of converting data from nominal format to polynomial format. We connect our dataset to this operator. And in the parameter section of this operator, we put the attribute filter type as single. Here, we want to divide the age of people into three returns and cover all of them, and set the number of bins to 3. We use the setrole operator and the survival attribute as the feature that is going to be predicted. For the operator team, we name the attribute name Survived and the target role as the label that is going to be predicted. In the Attribute Roles section, we can define their roles for others. So that the software understands which attribute should be used in which direction and run. As you can see, the attribute "survived" has turned green and the ages were divided into three returns and included all the ages that were in our dataset. For more practice, we can go to discretize by binning to define two attributes. which are going to be zoned. We put the Attribute filter type on a subset and we can select age and passenger fare in the attribute selection section. And change the number of divisions that we previously set to 5 to 5. And let's run again to see what happened to the age and passenger fare columns - which we divided into 5 results.

5) In this section of Project 6: we are going to talk briefly about modeling. Here we use one of the most widely used machine learning models called decision tree. To predict who survived the Titanic ship accident, or what attributes are more likely to survive for people, the first step is to enter data from the sample section of the Titanic. In the second step, we are working with operator setrole, and we set the survival attribute as a label. So that our modeling does not suffer from errors. And we choose the label for the roll. Machine learning methods such as decision trees use existing data to find hidden patterns. Then it discovers a possible pattern and predicts the new data set which is unlabeled according to the constructed pattern. The select attribute operator is one of the most important operators of this software. If we want to remove some attributes and include those used in modeling in our process, we must use the select attribute operator. Here we have to remove three attributes. Attributes
lifeboat: If the algorithm wants to decide according to this attribute because they are alive, and it wants to predict according to this attribute, and, an unknown attribute-Name-ticket number, these two are IDs and did not interfere with people being alive. So we select all attributes except these three. The next step is to choose our modeler. In the same decision tree, we enter this modeler into the environment. After connecting the ports, we select their parameters. The decision tree algorithm starts from the beginning and by calculating a criterion, it calculates the amount of information that each attribute can provide according to the ratio of the classes of that attribute. This criterion can be gain-ratio, the maximum depth to advance, and the criterion value for accepting that attribute for branching is determined as the minimum gain in this part of the operator's parameters. We can change any of these numbers as we wish, and if we have a user account, it will show us the best value of these parameters in each of them. Here we leave the default values that are probably used the most. We connect from Chort Mod to Rizalt. Of course, we can connect the example port to the result here and run it. We can see the tree and the example. Here we see that most of the gentlemen died and their survey was no. In the next branch, we see that it is divided according to gender, and then according to other attributes, which continues to the bottom. By analyzing the decision tree, we find that the middle branch has the highest chance of surviving.

Handle missing values: It is considered one of the most important parts of working with data.
Data handling is divided into two parts.
Lending and cleansing: including removing or replacing missing values or normalization
Deleting or replacing missing values consists of three parts. Deleting some instances - relisting missing values, deleting attributes completely
1- Project 7 and 8: We import the Titanic dataset into the environment. If we put the icon on the output port, it will give us the information, and by pressing the F3 key, we can easily see the content, as well as the characteristics of the lifeboat and cabin. Even if missing values are replaced with other numbers such as average data, attribute, or mode, they will still cause errors in the sentence, so in the first step, this attribute should be removed from our dataset To make our modeling more suitable, we will use the select attribute and in this part, we want to have a subset of attributes, except for life bot and Now we connect the port and see how the number of missing values has decreased and only 263 missing attributes are left. of embarkation
Project 8: This time we want to place the missing values of the age attribute. Then we use the replace missing value operator. Here we leave the filter type attribute single. And we leave the attribute boundary as age. We run here to see what has changed this time. It is zero for all attributes, except in three cases where we decide to remove these to have a complete and proper dataset. To completely remove those three examples from our dataset. We use the filter example operator. And in the parameters section, we want to say those instances that have missing attributes. be deleted. For the condition class, we set no missing attribute and run. Now, in the statistics section, we can see that there is no missing value for any of the attributes and examples. In this way, we have a complete dataset. If we completely remove all the attributes that have missing values, our modeling will be wrong. If we replace all the missing values, our model will still have problems. Therefore, we must pay attention to the number of missing elements in each attribute in each dataset compared to the total data, so that we can make the best decision and finally create the best model.

2- Project 9: One of the parts of data handling includes normalization. In this part, we remove outlier data from our dataset to create a better model, for example, the outlier data that takes place in a transaction bank operation.
Data handling is divided into two parts: 1. Lending and 2.: Cleansing includes two parts, removing or replacing missing values or normalization.
Removing or replacing missing values includes three parts: deleting some instances, replacing missing values, and deleting attributes completely.
In the first step, we enter Titanic's sample data into the environment. If we hold the sample data on the output port and press F3, it will give us the information and attributes. We use the select attribute operator and enter the Rapidminer environment. In the attribute field of the type filter, we wanted to have this subset of attributes, all of them should be included in the attribute and dataset, except for lifeboat and cabin.
We can take a run to see how our number of missing values has changed.
In the statistics section, we can see the missing values, and their number has decreased significantly, and only the age attribute remains with 263 and three more examples in the passenger fare and port of embarkation attributes. We return to the design section, this time we want to place the missing attributes of the age attribute. So, we use the replace missing values operator. In this part, we set the type filter attribute to single and the age attribute itself.
I ran it again, and we can see that everything is enabled except for the three previous cases, in this part we recognize that we need to delete some examples to have a complete and suitable dataset for modeling, so we use the filter example operator to completely delete those three examples. For the parameter part of this operator, we want to say that those instances that have the missing attribute should be deleted. In this part, we click on show advanced parameter and the condition class will be shown to us, we select the no missing attribute option, that is, no instance with the missing attribute. it doesn't exist and I run it and we see that there are no missing values left for any of our attributes and instances, in this way, we have a complete dataset. Here it is important to identify which features to delete and which ones to replace if we want to have all the attributes that have missing values, we should leave them in general, our modeling will go wrong, and if we want to replace all the missing values, our model will still have problems, so we must pay attention to the number of missing values in each attribute in each dataset compared to the total data. How can we make the best decision and finally have the best modeling?
3- Project 10: One of the parts of cleansing or data handling includes normalization. In this part, we remove outlier data from our dataset to create a better model, for example, outlier data that occurs in a banking operation or transactions is identified and the suspicious case is reported, but in most cases, outlier data were entered due to operational or typographical errors and they can overshadow the modeling with this simple mistake, so it is better to remove them in this part of data normalization because All attributes are converted to a scale and it becomes easier to compare the range of data. Therefore, an Euclidean distance is defined for each attribute and the data is measured against that distance. If they are in that range, they are considered normal data, and if they are not in that range They are considered outlier data and cause modeling problems, so we enter the Titanic dataset into the environment for normalization, we know that numerical attributes can be normalized, so nominal attributes and attributes that have missing features should be removed at the beginning of the work, so We use the select attribute operator before connecting to the output port to know which attributes should remain and which should be deleted, all attributes should be deleted except age, no of siblings, no of parents, ticket number, passenger fare, lifeboat

Of course, the boat should be removed because it has a lot of missing values, so click on the selected operator select the subset in the attribute filter type section, and select the attributes as we said. All values are normalized, and finally, all are expressed as standard normal with a mean of 0 and a standard deviation of 1. In this way, the average of each attribute is calculated and after that, the data value minus the average of that attribute is divided by the standard deviation of that attribute, and all our examples with each attribute are normalized in the same way, we use the normalization operator. which normalizes all our attributes and here we can get an output and here we get a khajoji. But all our data became normal. In this part, we can set the type filter attribute as a subset. We must pay attention.
The ticket number is known by the software, so the normalization of the method has not been done and we should have removed this part from the beginning. We go back to the design part return the selected attribute and ticket number to the previous part and apply it so that our modeling is not wrong. In the next step, we must use the detect outlier attribute. The number of neighbors and the number of outliers specifies how many outliers should be identified. Their choice is up to, as well as the number of attributes neighbors, it's up to us, as well as the type of data distance calculation, and those normal values that we choose Euclidean distance. If we run one more time. We can see that a column called outlier is shown in green, which contains false and true values. True is the place where that attribute is set. We return to the design environment and discard those examples that have outliers. We use the exemplar filter operator. Here, the same 10 data specified by us will be removed to identify outliers. We set the filter example here, outlier equals false
: 
That is, show us data that does not have outliers and we will run. Finally, we have 1299 normalized data and we removed the outlier data and in this way, we increased the quality of modeling with this type of data. We return to the design environment and set our filter to true in the filter section of Exempb.

4- Project 11 and 12. Then we can see the outlier data together. These 10 items are known as outlier data in the attribute. They were examples that had outlier data in one of their attributes. Likewise, in the design environment, we can right-click on the attribute detect outlier: breakpoint after, then the step will be run. It shows the step of how outliers are determined, we run it and in this part, we mark the blue arrow and finally, we will have this dataset. In this part, one of the normalization techniques is called pivoting: in fact, we want the data of a table. Convert a long table to a wide table. A long table is a table with several instances and one attribute, but a wide table is a table that has a large number of attributes of one instance. This is used to avoid confusion among users. This conversion is used after aggregation. be where the information is aggregated and all the aggregated information is stored as a long table, while ml models need wide columns and the tables must be converted to the wide table format, so you can use this pre-processing before each modeling. In the first step, we want to enter our data. We bring the Titanic dataset, then we want to know how many men and women there are in each class, and then we use the aggregate operator. To determine aggregate parameters, classification is to be done based on two characteristics. So, we select the group attribute and select two attributes passenger class and sex. Then we enter the aggregation attribute section click edit list and click passenger class for us to count. It's time to run once, as you can see, we have a table with three features and 6 lines, in each line, passenger class and sex are combined, and their number is listed in the last column, for example, women who had passenger class 1. There were 144 people. This table is long. We want to convert this to a wide table to make it clearer for us, so we go back to the design environment and add the pivot operator to our environment. We have to choose the parameters of this operator group attribute: which attribute should be classified and grouped and select sex. Index attribute refers to what attribute our columns carry and we put this as passenger class. As you can see, the information became more understandable. The problem with this is that the names of our columns are newly created and come with an unrelated names, so in the next step, we want to change them to our desired name. We can make this change manually with a rename.
Rename is perfect when the number of attributes is small, but there are more advanced techniques for more attributes, so let's go back to the design environment and enter the rename by replacing the operator. In the field replace what: count\((.*)\)_(.*) and in the field replace by $1 we enter $2.

* shows us that these are words and are considered phrases, and then parentheses are used to group names, and the backslash is for this group of names to count as one phrase and not to see two separate words. We also added _ because the title of the table is also included and we added the second group of names in parentheses: first,... and in the next part we are saying that those phrases should be replaced with the first group of names next to the second group of names, that is, anything else other than these. Remove only these two groups of names
We have to remember that in the first stage of grouping, we used the aggregate operator to group based on two items. In the next stage, i.e. pivot, we considered the columns as passenger class. Rapidminer is case-sensitive, punctuation and space-sensitive. Therefore, be careful when writing phrases. We right-click on the aggregate operator and set a breakpoint after. to see step-by-step what is happening to our process. And by clicking on the blue arrow in the result section for the second step, we can see the results.

5- Project 13, working with macros: there are variables that you can use to dynamically load and store values to reduce the size of macros by 50% and the macro calculates the new size of our dataset and half of the data To show us, in the first part, we enter our data, in the second step, we use the set macro operator to enter our environment.
In the macro parameters section, its name and the value to be assigned to the macro should be set to 0.5 because we want it to show us 50% of the data. The next step is to use the extract macro operator to find out what the macro is being done on. We were supposed to define macro and macro type on the size of our data
as several examples. We used the attribute generator operator for new operators. Here too we can use generate macro. By multiplying the value of the fraction, which was half, by the old size of the data that we defined in the extract macro, we will finally get the rand of this value and this will be the new size of our dataset, so the next step is to generate a macro with the relevant operator, and here we want our function discrimination create.
The new name of the macro should be new size and the functions expressions part should say how to define new size round(eval(%{size})*eval(%{fraction})) which our initial value returns the numerical value of the initial dataset size and our second value It also takes into account the fraction that we initially defined as 0.5 for set macro.
After this, we want to define that it should do a sampling of what we created and give us 50% of our data, so in the next step, we enter the sample operator. To define the parameters of the sample operator, the sample part must be absolute, and in the second part we must show the sample size, we enter the expression new size to return that numerical value to us, and now we run.
We have 655 copies here. The macro worked correctly, the sampling was correct, and provided us with 50% of the data. We can change our macro. So we set the value of the macro operator to 0.2 and this changes the entire process. You can see that he brought us 263 copies. If we want larger numbers, for example, 500 of our samples, select and bring them from the dataset. Now round(eval(%{fraction})) is defined, we apply and run. As you can see, 500 samples were filtered and included in our dataset.
